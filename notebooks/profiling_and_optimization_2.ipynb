{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearnex.cluster import DBSCAN\n",
    "from imutils.video import FileVideoStream\n",
    "np.random.seed(42)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "np.seterr(divide='ignore') # ignore divide by zero when calculating angle\n",
    "%load_ext line_profiler\n",
    "\n",
    "# parameters for Hough Line detection\n",
    "RHO = 1                 # distance resolution in pixels of the Hough grid\n",
    "THETA = np.pi / 180     # angular resolution in radians of the Hough grid\n",
    "THRESHOLD = 20          # minimum number of votes (intersections in Hough grid cell)\n",
    "MIN_LINE_LENGTH = 5     # minimum number of pixels making up a line\n",
    "MAX_LINE_GAP = 2        # maximum gap in pixels between connectable line segments\n",
    "WIDTH = 720             # width to resize the processed video to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9ec1b",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "1. Use `Numpy` instead of `Pandas` for processing the centroid data\n",
    "2. Use `zarr` library to write and append processed frame data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized(fname, save_video=False, savename=None, show_video=False, save_stats=False,\n",
    "                    frame_limit=False):\n",
    "    if savename == None:\n",
    "        savename = \"saber_tracking.avi\"\n",
    "\n",
    "    if save_video:\n",
    "        # Initialize video writer to save the results\n",
    "        out = cv2.VideoWriter(savename, cv2.VideoWriter_fourcc(*'XVID'), 30.0, \n",
    "                                 (WIDTH, WIDTH), True)\n",
    "\n",
    "    cap = cv2.VideoCapture(fname)\n",
    "    ret, frame = cap.read()\n",
    "    total_frames = 500 if frame_limit else int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    pbar = tqdm(total=total_frames)\n",
    "    frame_num = 0\n",
    "\n",
    "    output_path = savename.replace(\".avi\", \"_data.csv\")\n",
    "    # prevent appending to existing file\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "\n",
    "    # instantiate DBSCAN for use throughout\n",
    "    # n_jobs parallelisation introduces too much overhead\n",
    "    db = DBSCAN(eps=5, min_samples=2)\n",
    "    data = np.array([])\n",
    "\n",
    "    while ret:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, (frame.shape[1] // 2, frame.shape[0] // 2))\n",
    "            # these channels were swapped in the notebook\n",
    "            b = cv2.inRange(frame[:, :, 2], 200, 255)\n",
    "            r = cv2.inRange(frame[:, :, 0], 180, 255)\n",
    "\n",
    "            # convert to HSV for more masking options\n",
    "            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            v = cv2.inRange(hsv[:, :, 2], 170, 255)\n",
    "            s = cv2.inRange(hsv[:, :, 1], 140, 175)\n",
    "\n",
    "            # combine masks into one\n",
    "            m1 = cv2.bitwise_and(b, s)\n",
    "            m2 = cv2.bitwise_and(r, v)\n",
    "            mask = cv2.bitwise_or(m1, m2)\n",
    "\n",
    "            # Run Hough on masked image\n",
    "            # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "            lines = cv2.HoughLinesP(mask, RHO, THETA, THRESHOLD, np.array([]), MIN_LINE_LENGTH, MAX_LINE_GAP)\n",
    "\n",
    "            # process lines\n",
    "            if isinstance(lines, np.ndarray):\n",
    "                lines = np.squeeze(lines)\n",
    "                cx = lines[:, :2].sum(axis=1).reshape(-1, 1)\n",
    "                cy = lines[:, 2:].sum(axis=1).reshape(-1, 1)\n",
    "                frames = np.zeros(cx.shape) + frame_num\n",
    "                slopes = (lines[:, 3] - lines[:, 1]) / (lines[:, 2] - lines[:, 0])\n",
    "                angles = np.rad2deg(np.arctan(slopes)).reshape(-1, 1)\n",
    "                lengths = np.linalg.norm(lines[:, :2] - lines[:, 2:], axis=1).reshape(-1, 1)\n",
    "                data = np.concatenate((frames, cx, cy, angles, lengths), axis=1).round(3)\n",
    "                \n",
    "                # filter by edge conditions and line length\n",
    "                xedge_mask = np.logical_and(data[:, 1] > 100, data[:, 1] < 540)\n",
    "                yedge_mask = np.logical_and(data[:, 2] > 50, data[:, 2] < 310)\n",
    "                len_mask = np.logical_and(data[:, -1] > 10, data[:, -1] < 50)\n",
    "                mask = np.logical_and(len_mask, xedge_mask)\n",
    "                mask = np.logical_and(mask, yedge_mask)\n",
    "                data = data[mask]\n",
    "\n",
    "            # perform clustering to reduce data\n",
    "            if data.size > 0:\n",
    "                db.fit(data[:, 1:4])\n",
    "                data = np.concatenate((data, db.labels_.reshape(-1, 1)), axis=1)\n",
    "                data = data[data[:, -1] != -1]\n",
    "                if data.size > 0:\n",
    "                    df = df.groupby(\"labels\", as_index=False, sort=False).mean()\n",
    "                    for cx, cy in zip(df[\"centroid_x\"], df[\"centroid_y\"]):\n",
    "                        cv2.drawMarker(frame, (int(cx), int(cy)), (0, 255, 0), markerType=cv2.MARKER_CROSS, thickness=2)\n",
    "\n",
    "            if save_stats:\n",
    "                # save or append numpy array to file\n",
    "                df.to_csv(output_path, index=False, mode='a',\n",
    "                            header=not os.path.exists(output_path))\n",
    "\n",
    "                # reset data structure\n",
    "                data = np.array([])\n",
    "\n",
    "            resized = cv2.resize(frame, (WIDTH, WIDTH))\n",
    "\n",
    "            if show_video:\n",
    "                cv2.imshow(\"Frame\", frame)\n",
    "            if save_video:\n",
    "                out.write(resized)\n",
    "            frame_num += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            if key == ord('p'):\n",
    "                cv2.waitKey(-1) # wait until any key is pressed\n",
    "            if frame_limit and frame_num == 500:\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    if save_video:\n",
    "        out.release()\n",
    "    if show_video:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd643001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f optimized \n",
    "optimized(\"../test_video.mp4\", False, \"min_length_30.avi\", False, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dab9ec",
   "metadata": {},
   "source": [
    "## Optimization testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a044bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = np.abs(np.random.normal(150, 60, size=(5,4)))\n",
    "\n",
    "def np_line_proc(lines):\n",
    "    frame_num = 0\n",
    "    cx = lines[:, :2].sum(axis=1).reshape(-1, 1)\n",
    "    cy = lines[:, 2:].sum(axis=1).reshape(-1, 1)\n",
    "    frames = np.zeros(cx.shape) + frame_num\n",
    "    slopes = (lines[:, 3] - lines[:, 1]) / (lines[:, 2] - lines[:, 0])\n",
    "    angles = np.rad2deg(np.arctan(slopes)).reshape(-1, 1)\n",
    "    lengths = np.linalg.norm(lines[:, :2] - lines[:, 2:], axis=1).reshape(-1, 1)\n",
    "    data = np.concatenate((frames, cx, cy, angles, lengths), axis=1).round(3)\n",
    "    \n",
    "    # conditions\n",
    "    # edge_x = 100 < centroid[0] < 540\n",
    "    # edge_y = 50 < centroid[1] < 310\n",
    "    # l = 50 > length > 10\n",
    "    xedge_mask = np.logical_and(data[:, 1] > 100, data[:, 1] < 540)\n",
    "    yedge_mask = np.logical_and(data[:, 2] > 50, data[:, 2] < 310)\n",
    "    len_mask = np.logical_and(data[:, -1] > 10, data[:, -1] < 50)\n",
    "    mask = np.logical_and(len_mask, xedge_mask)\n",
    "    mask = np.logical_and(mask, yedge_mask)\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.linalg.norm(lines[:, :2] - lines[:, 2:], axis=1).reshape(-1, 1)\n",
    "\n",
    "@nb.njit(parallel=True)\n",
    "def nb_line_proc(lines, lengths):\n",
    "    frame_num = 0\n",
    "    cx = lines[:, :2].sum(axis=1).reshape(-1, 1)\n",
    "    cy = lines[:, 2:].sum(axis=1).reshape(-1, 1)\n",
    "    frames = np.zeros(cx.shape) + frame_num\n",
    "    slopes = (lines[:, 3] - lines[:, 1]) / (lines[:, 2] - lines[:, 0])\n",
    "    angles = np.rad2deg(np.arctan(slopes)).reshape(-1, 1)\n",
    "    data = np.concatenate((frames, cx, cy, angles, lengths), axis=1)\n",
    "    \n",
    "    # conditions\n",
    "    # edge_x = 100 < centroid[0] < 540\n",
    "    # edge_y = 50 < centroid[1] < 310\n",
    "    # l = 50 > length > 10\n",
    "    xedge_mask = np.logical_and(data[:, 1] > 100, data[:, 1] < 540)\n",
    "    yedge_mask = np.logical_and(data[:, 2] > 50, data[:, 2] < 310)\n",
    "    len_mask = np.logical_and(data[:, -1] > 10, data[:, -1] < 50)\n",
    "    mask = np.logical_and(len_mask, xedge_mask)\n",
    "    mask = np.logical_and(mask, yedge_mask)\n",
    "    return data[mask]\n",
    "\n",
    "_ = nb_line_proc(lines[:2], lengths[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_line_proc(lines):\n",
    "    frame_num = 0\n",
    "    cx = lines[:, :2].sum(axis=1).reshape(-1, 1)\n",
    "    cy = lines[:, 2:].sum(axis=1).reshape(-1, 1)\n",
    "    frames = np.zeros(cx.shape) + frame_num\n",
    "    slopes = (lines[:, 3] - lines[:, 1]) / (lines[:, 2] - lines[:, 0])\n",
    "    angles = np.rad2deg(np.arctan(slopes)).reshape(-1, 1)\n",
    "    lengths = np.linalg.norm(lines[:, :2] - lines[:, 2:], axis=1).reshape(-1, 1)\n",
    "    data = np.concatenate((frames, cx, cy, angles, lengths), axis=1).round(3)\n",
    "    \n",
    "    xedge_mask = cv2.inRange(data[:, 1], 100, 540)\n",
    "    yedge_mask = cv2.inRange(data[:, 2], 50, 310)\n",
    "    len_mask = cv2.inRange(data[:, -1], 10, 50)\n",
    "    mask = cv2.bitwise_and(len_mask, xedge_mask)\n",
    "    mask = np.squeeze(cv2.bitwise_and(mask, yedge_mask)) > 0\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fedf773",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np_line_proc(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit nb_line_proc(lines, lengths)\n",
    "%timeit np.linalg.norm(lines[:, :2] - lines[:, 2:], axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cv2_line_proc(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02479295",
   "metadata": {},
   "source": [
    "## Results\n",
    "* Using OpenCV to mask line data doesn't perform much better than numpy (~35-36 microseconds)\n",
    "* Tetsing `Numba` on this function requires the removal of `np.linalg.norm` from the function since they don't currently support the axis argument in this function. \n",
    "<strong>This is an open issue on their [Github](https://github.com/numba/numba/pull/7785)</strong>, but it reduces the time to ~8 microseconds even when moving the `norm` function outside the \"jitted\" function -> 4.5x speedup\n",
    "However, this will only save 1 second when processing the full 33,838 frame video used for testing, so I'll skip for now\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f987404",
   "metadata": {},
   "source": [
    "## Examine optimized profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_profile_as_df(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    arr = []\n",
    "    for l in lines:\n",
    "        if l.startswith(\"   \"):\n",
    "            split = [i.strip() for i in l.split(\"   \") if i.strip() != \"\"]\n",
    "            if len(split) == 6:\n",
    "                arr.append(split)\n",
    "        elif l.startswith(\"Line\"):\n",
    "            columns = [i.strip() for i in l.split(\"  \") if i != \"\"]\n",
    "    prof = pd.DataFrame(arr, columns=columns)\n",
    "    # set correct data types\n",
    "    prof = prof.astype({\"Line #\" : \"uint16\", \"Hits\" : \"uint16\",\n",
    "            \"Time\" : \"float64\", \n",
    "             \"Per Hit\" : \"float16\",\n",
    "            \"% Time\" : \"float16\"})\n",
    "    return prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f27c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = read_profile_as_df(\"../profiles/optimized_unlimited_profile.txt\")\n",
    "\n",
    "top = 10\n",
    "perc = 1\n",
    "largest = prof[\"% Time\"].nlargest(10)\n",
    "sub = prof.iloc[largest[largest >= perc].index, :]\n",
    "print(f\"Of the top {top} most time-consuming lines,\")\n",
    "print(f\"{sub.shape[0]} are >= {perc} % for {sub['% Time'].sum():.1f} % of the time taken\")\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86030de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prof = read_profile_as_df(\"../profiles/optimized_fvs_unlimited_profile.txt\")\n",
    "\n",
    "largest = prof[\"% Time\"].nlargest(10)\n",
    "sub = prof.iloc[largest[largest >= perc].index, :]\n",
    "print(f\"Of the top {top} most time-consuming lines,\")\n",
    "print(f\"{sub.shape[0]} are >= {perc} % for {sub['% Time'].sum():.1f} % of the time taken\")\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
